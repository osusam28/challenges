# Log Processing Project Implementation

## Log Processing Code

### Setup/Requirements

Python 2/3 installed (I used 2 because that's what my system has installed at the moment, but would recommend using 3).  There is a great installation guide here if you do not have it installed: https://cloud.google.com/python/setup#installing_python

### Assumptions

For simplicity's sake, I have made some assumptions about the log data.  Please let me know if the purpose of this exercise was to not make these assumptions

* The log file will always have the header row and the order/name of the fields will not vary from the sample log file
* The header row will be the first line of the file
* The delimeter used in the sample log file will be the one used for all the log files
* The log file will have at least one line
* The output file does not need to include logs without a 200 or 206 response


### Unit Test Execution

The unit tests will run standalone by running the script `parser_test.py` (e.g. `python parser_test.py`).

These tests are minimal, but meant to show how a script/app like this can be unit tested

### Execution

The file `parser.py` contains the code that does the processing.

The filepath to the log file to process must be passed in as an argument to the python script `parser.py`. (e.g. `python parser.py sample.log`). This filepath can be relative or absolute.

An optional path to an output file can be passed in as a second argument (e.g. `python parser.py sample.log sample_output.json`).  This will generate an output file that can be used for ingestion into elasticsearch using the bulk API. The output file type should be `.json`. If this argument is not passed in, the output will be sent to the console.

A sample output file is included as `sample_output.json`

## Integration Testing (with Elasticsearch)

### Setup/Requirements

**Install Docker**: https://docs.docker.com/get-docker/

**Run Elasticsearch locally**: Execute the shell script `setup/run_elastic.sh`.  This will download the Elasticsearch docker image onto your local machine and begin a local instance of it

**Upload processed log data**: Execute the shell script `setup/populate_elastic.sh` with a single argument of the file generated by the log processing script in the previous section (e.g. `bash populate_elastic.sh output.json`).  This uploads all the processed data into the Elasticsearch instance.

### Querying 

A sample query is located at `requests/request.json`.  This query can be executed using a script like in `requests/query_elastic.sh`. Sample results of the query are also in the same location at `requests/results.json`.

The query represents a request of whether bytes 150-500 were delivered to the ip address, user agent, and asset request. It will return the log records that have a byte range that includes or is larger than 150, all the way up to the byte range that includes 500. Based on the records that are returned for this query, an algorithm can be written to determine whether any bytes were missing in the range. For instance, the following records might be returned for the aforementioned query (just the byte range is shown):

| record id | max byte | max byte |
|-----------|----------|----------|
| 1         |112       |149       |
| 2         |149       |224       |
| 3         |224       |344       |
| 4         |467       |515       |

An algorithm to check whether the returned sequence of bytes is continuous and whose min < 150 and max > 500 would return the desired result.  In the above case it is clear that bytes 344-467 were missed, and so a query for bytes 150-500 would return False.

> **Note**: the script `requests/mock_client` simulates the response of a client based on the byte ranges returned in the above table. The script takes two arguments, the minimum value of the byte range query, and the maximum value of the byte range query. For instance, `python mock_client.py 150-500`. 

# Log Processing Project Discussion

## Database Selection

Database selection varies greatly based on the use cases.  The two broad use cases that projects normally fall into are either analytical or transactional (operational). Analytical use cases require fast processing and aggregation of large datasets (which can be TB in size), but normally don't require large quantities of concurrent queries or sub-second responses. They also are generally used to return a large set of data to the client, such as a dataset used for training a machine learning model. Transaction use cases usually require very low latencies and have to sometimes support hundreds or thousands of queries per second on largge datasets, however they generally are only returning a few records to the client.  An example of this would be an API that returns a single customer's information from a datastore of all customers. 

The use case presented in this project seems to be primarily a transactional one, as it seems that clients will be making individual queries for different byte ranges based on specific filtering of the log records. 

One of the important things to consider in this project is the fact that each query will be required to filter over several fields (request, ip_address, user_agent), and so any database that I use will need to be able to efficiently filter on all those fields.

I selected Elasticsearch as the database for this particular project based on:

1. Its ability to be a scalable, transactional database. Would work with 1 log record or 1 billion log records.
2. It requires minimal data transformation to ingest and efficiently query because it is based on the Lucene inverted index architecture
3. Easy to setup and well documented, as well as open source (free), so it can be run on a local server, or in the cloud
4. Has integrations and native tooling setup to ingest logs at a enterprise scale if I ever decided to move away from my custom python log processor

Some downsides of Elasticsearch:

1. Could be overkill for a use case involving only a small volume of logs
2. Requires technical knowledge of the open source technology, else you pay a large fee to use a managed Elasticsearch service
3. Query language has a steep learning curve
4. Write operations can be less reliable than other solutions

Other alternatives to considered:

1. MongoDB (or any transaction NoSQL database): This is a strong contender that I could very easily be pursuaded into using. It can ingest JSON data much like Elasticsearch, and is cheaper to use.  It does not have the inverted index structure that Elasticsearch has, so it might take some more design to make the queries for this as efficient because of the number of fields on which each query will need to filter. 
2. Google Bigtable (or HBase): If we knew that the volume of logs would quickly expand into hundreds of TB and needed a fast and scalable way to give many clients the abilily to find whether certain bytes were delivered, this could be the solution. We would need to develop a key structure (Bigtable only indexes on one key) that would include all fields to filter on, but this database's performance is second to none. The downside is that it is very expensive, and requires some thoughtful data architecture. 
3. Google BigQuery (or Redshift/Cassandra): This database would only be useful if the queries for specific byte ranges were not exepcted to be concurrent or frequent (this is an analytical database). However the cost of this database would be much less than maintaining a transactional database, and the ingestion into it would be more flexible, as well as the advantage of being able to use standard SQL to query.

## Data Structure

I simply parsed the data from each log and assigned it to a corresponding field in a JSON representation.  The only additional transfomration was splitting the byte range into a min_byte and max_byte field. Because of Elasticsearch's inverted index structure and query language, no other transformation was needed and the data could be ingested and queried. 

## Enterprise (Production) Solution

To build a comprehensive production solution for this byte range log analyzer, we would need to consider the source of the data, the ingestion/transformation pipeline, as well as the consumer of this data.

### Source Consideration

In this project, the source of the logs was a file.  It could be that the file was produced by an application which is the true source of the logs.  I would want to consider whether connecting directly to, and streaming logs from, the application itself would be the best way to ingest the data. 

### Ingestion Consideration

Depending on the source, an ingestion pipeline would need to be setup for the logs into the database of choice (Elasticsearch in this instance). No matter the source of the logs, the pipeline would need to ensure that each log was delivered to its final destination. This could be done realtime (such as via a queueing system where the log is not perminantely removed from the queue until it has been successfully processed), or at a later time (such as through a reconciliation batch job that would look at a log archive and compare against the target databse). Either way, an archive of the log data would probably be good to maintain as a source of truth outside of the database used for querying. 

This pipeline could consist of multiple steps (source > queue > transformation > queue > database), or a single step (source > database). The choice would be dependent on the requirements and needs of the business.

The ingestion pipeline would most likely need to be scaleable to handle fluctuations in incoming log volume. 

### Consumer Consideration

The database which will be queries and the data structure within it need to be designed with the consumer's use case in mind, as well as future use cases and future growth.  However, it is easy to fall into the pit of over-designing and building for use cases that may or may not be realistic. Therefore, though scaleability, maintainability, and resiliency should be forefront when designing an entire production system, it should also not be so generic that it fails to provide the value needed for the business and consumer.
